---
title: "Analysis of Activity Logs"
author: Doug Davidson
date: June 10, 2018
output: 
  html_document:
  keep_md: true
---

```{r setup}
# Library setup
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(caret)
library(rpart.plot)
```

### Synopsis

This analysis uses the data from accelerometers placed on the belt, forearm, arm, and dumbell of six different participants to predict how well they performed a biceps curl. In the training data set, the performance was labeled as correct ("A") or mistaken ("B":"E"). The result of the analysis was completely inconclusive -- none of the models obtained successful prediction.


### Data pre-processing

The training and testing data sets were read in while assigning blank entries a missing value.  Some variables were largely missing values, subject-id, or time-stamp values, and were therefore dropped.

```{r explore}
# Data read-in
dat <- read.csv("pml-training.csv", na.strings = c("NA", ""))
tst <- read.csv("pml-testing.csv", na.strings = c("NA", ""))

# Eliminate columns that are mostly NA
dat <- dat[, -which(colMeans(is.na(dat)) > 0.75)]
tst <- tst[, -which(colMeans(is.na(tst)) > 0.75)]

# Exclude some columns, but keep subject ID and time-stamp variables
dat <- dat[,c(2:3,8:60)]
tst <- tst[,c(2:3,8:60)]

# Dimensions
dim(dat)
dim(tst)
```

The original training data set ('dat') was then partitioned into a training ('trn') and an evaluation ('vld') data set so that we can get some idea of the out-of-sample error.

```{r split}
# Partition 
set.seed(666)
intrn <- createDataPartition(y = dat$classe,
                             p = 0.75,
                             list = FALSE)
trn <- dat[intrn,]
vld <- dat[-intrn,]
vald <- dat[-intrn,]

preObj <- preProcess(trn, method=c("center", "scale"), thresh=0.9)

trnRsp <- predict(preObj, trn)
vldRsp <- predict(preObj, vld)
tstRsp <- predict(preObj, tst)

```

The methods of linear discriminative analysis (LDA), classification and regression trees (CART, or rpart), and random forest (RF) were used to train classifiers for the outcome.


```{r train}

# Training LDA
m1 <- caret::train(classe~.,
                   method="lda", 
                   data=trnRsp)

# Training CART
m2 <- caret::train(classe~.,
                   method="rpart", 
                   data=trnRsp)

# Plot of CART model
rpart.plot(m2$finalModel)

# Training Random Forest
m3 <- caret::train(classe~.,
                   method="rf", 
                   data=trnRsp)
```

The LDA model did not complete because of warnings of collinearity of the variables, so we will concentrate on models 2 (CART) and 3 (Random Forest).

A tree plot for the CART model suggested a problem in that category D was not used.

To evaluate performance, predictions were made for the held-out evaluation data set. Unfortunately, LDA only predicted "D" (accuracy: 0.16), CART favored only "E" and only sometimes "A" or "B" (accuracy: 0.22), and RF only predicted "B" or "E" (accuracy: 0.17).  It is not obvious why the models failed in this way, but it may be that I missed some important options to specify from the lectures.


```{r eval}
# Test
v1 <- predict(m1,newdata=vld)
v2 <- predict(m2,newdata=vld)
v3 <- predict(m3,newdata=vld)

confusionMatrix(v1, vld$classe)
confusionMatrix(v2, vld$classe)
confusionMatrix(v3, vld$classe)
```

The accuracy for the *training* set was relatively high for the Random Forest method (0.95), but not for the CART model (0.53).  Given the poor out-of-sample prediction, however, it seems both RF and CART essentially failed.

Finally the predicted outcomes for the original test data set were calculated using the models based on the training data. 

```{r predict}
# Prediction
p1 <- predict(m1, newdata=tst)
p2 <- predict(m2, newdata=tst)
p3 <- predict(m3, newdata=tst)
print(p2)
```

The same problems found with the validation data set were found with the test set.  Ultimately, none of the models provided a successful prediction.  It seems likely that I missed some important options for these functions.


```{r session_info,eval=TRUE}
sessionInfo()
```




